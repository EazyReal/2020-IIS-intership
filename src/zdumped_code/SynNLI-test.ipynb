{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynNLI v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor #\n",
    "import allennlp_models.structured_prediction\n",
    "import allennlp_models.coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.instance import Instance\n",
    "\n",
    "from typing import Iterable\n",
    "import logging\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_srl = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbs': [{'verb': 'think', 'description': 'Did [ARG0: Uriah] [ARGM-MNR: honestly] [V: think] [ARG1: he could beat the game in under three hours] ?', 'tags': ['O', 'B-ARG0', 'B-ARGM-MNR', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O']}, {'verb': 'could', 'description': 'Did Uriah honestly think he [V: could] beat the game in under three hours ?', 'tags': ['O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'beat', 'description': 'Did Uriah honestly think [ARG0: he] [ARGM-MOD: could] [V: beat] [ARG1: the game] in [ARGM-TMP: under three hours] ?', 'tags': ['O', 'O', 'O', 'O', 'B-ARG0', 'B-ARGM-MOD', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O']}], 'words': ['Did', 'Uriah', 'honestly', 'think', 'he', 'could', 'beat', 'the', 'game', 'in', 'under', 'three', 'hours', '?']}\n"
     ]
    }
   ],
   "source": [
    "doc = predictor_srl.predict(\n",
    "  sentence=\"Did Uriah honestly think he could beat the game in under three hours?\"\n",
    ")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*weight_hh.*\n",
      "Did not use initialization regex that was passed: .*projection.*weight\n",
      "Did not use initialization regex that was passed: .*weight_ih.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*projection.*bias\n",
      "Did not use initialization regex that was passed: .*bias_ih.*\n"
     ]
    }
   ],
   "source": [
    "predictor_dep = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_loss': 0.37905168533325195, 'tag_loss': 0.5370486974716187, 'loss': 0.9161003828048706, 'words': ['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?'], 'pos': ['SCONJ', 'PRON', 'VERB', 'NUM', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'VERB', 'PRON', 'NOUN', 'PUNCT'], 'predicted_dependencies': ['mark', 'nsubj', 'advcl', 'dep', 'dobj', 'tmod', 'advmod', 'aux', 'nsubj', 'root', 'dobj', 'dep', 'discourse'], 'predicted_heads': [3, 3, 10, 5, 3, 3, 10, 10, 10, 0, 10, 11, 10], 'hierplane_tree': {'text': 'If I bring 10 dollars tomorrow , can you buy me lunch ?', 'root': {'word': 'buy', 'nodeType': 'root', 'attributes': ['VERB'], 'link': 'root', 'spans': [{'start': 41, 'end': 45}], 'children': [{'word': 'bring', 'nodeType': 'advcl', 'attributes': ['VERB'], 'link': 'advcl', 'spans': [{'start': 5, 'end': 11}], 'children': [{'word': 'If', 'nodeType': 'mark', 'attributes': ['SCONJ'], 'link': 'mark', 'spans': [{'start': 0, 'end': 3}]}, {'word': 'I', 'nodeType': 'nsubj', 'attributes': ['PRON'], 'link': 'nsubj', 'spans': [{'start': 3, 'end': 5}]}, {'word': 'dollars', 'nodeType': 'dobj', 'attributes': ['NOUN'], 'link': 'dobj', 'spans': [{'start': 14, 'end': 22}], 'children': [{'word': '10', 'nodeType': 'dep', 'attributes': ['NUM'], 'link': 'dep', 'spans': [{'start': 11, 'end': 14}]}]}, {'word': 'tomorrow', 'nodeType': 'tmod', 'attributes': ['NOUN'], 'link': 'tmod', 'spans': [{'start': 22, 'end': 31}]}]}, {'word': ',', 'nodeType': 'advmod', 'attributes': ['PUNCT'], 'link': 'advmod', 'spans': [{'start': 31, 'end': 33}]}, {'word': 'can', 'nodeType': 'aux', 'attributes': ['VERB'], 'link': 'aux', 'spans': [{'start': 33, 'end': 37}]}, {'word': 'you', 'nodeType': 'nsubj', 'attributes': ['PRON'], 'link': 'nsubj', 'spans': [{'start': 37, 'end': 41}]}, {'word': 'me', 'nodeType': 'dobj', 'attributes': ['PRON'], 'link': 'dobj', 'spans': [{'start': 45, 'end': 48}], 'children': [{'word': 'lunch', 'nodeType': 'dep', 'attributes': ['NOUN'], 'link': 'dep', 'spans': [{'start': 48, 'end': 54}]}]}, {'word': '?', 'nodeType': 'discourse', 'attributes': ['PUNCT'], 'link': 'discourse', 'spans': [{'start': 54, 'end': 56}]}]}, 'nodeTypeToStyle': {'root': ['color5', 'strong'], 'dep': ['color5', 'strong'], 'nsubj': ['color1'], 'nsubjpass': ['color1'], 'csubj': ['color1'], 'csubjpass': ['color1'], 'pobj': ['color2'], 'dobj': ['color2'], 'iobj': ['color2'], 'mark': ['color2'], 'pcomp': ['color2'], 'xcomp': ['color2'], 'ccomp': ['color2'], 'acomp': ['color2'], 'aux': ['color3'], 'cop': ['color3'], 'det': ['color3'], 'conj': ['color3'], 'cc': ['color3'], 'prep': ['color3'], 'number': ['color3'], 'possesive': ['color3'], 'poss': ['color3'], 'discourse': ['color3'], 'expletive': ['color3'], 'prt': ['color3'], 'advcl': ['color3'], 'mod': ['color4'], 'amod': ['color4'], 'tmod': ['color4'], 'quantmod': ['color4'], 'npadvmod': ['color4'], 'infmod': ['color4'], 'advmod': ['color4'], 'appos': ['color4'], 'nn': ['color4'], 'neg': ['color0'], 'punct': ['color0']}, 'linkToPosition': {'nsubj': 'left', 'nsubjpass': 'left', 'csubj': 'left', 'csubjpass': 'left', 'pobj': 'right', 'dobj': 'right', 'iobj': 'right', 'pcomp': 'right', 'xcomp': 'right', 'ccomp': 'right', 'acomp': 'right'}}}\n",
      "\n",
      " ['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?'] [3, 3, 10, 5, 3, 3, 10, 10, 10, 0, 10, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "doc = predictor_dep.predict(\n",
    "  sentence=\"If I bring 10 dollars tomorrow, can you buy me lunch?\"\n",
    ")\n",
    "print(doc)\n",
    "print(\"\\n\", doc[\"words\"], doc[\"predicted_heads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n"
     ]
    }
   ],
   "source": [
    "predictor_coref = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_spans': [[0, 4], [5, 5], [7, 8], [10, 10], [10, 11]],\n",
       " 'antecedent_indices': [[0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4]],\n",
       " 'predicted_antecedents': [-1, -1, -1, 0, -1],\n",
       " 'document': ['The',\n",
       "  'woman',\n",
       "  'reading',\n",
       "  'a',\n",
       "  'newspaper',\n",
       "  'sat',\n",
       "  'on',\n",
       "  'the',\n",
       "  'bench',\n",
       "  'with',\n",
       "  'her',\n",
       "  'dog',\n",
       "  '.'],\n",
       " 'clusters': [[[0, 4], [10, 10]]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_coref.predict(\n",
    "  document=\"The woman reading a newspaper sat on the bench with her dog.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DatasetReader.register('nli-jsonl')\n",
    "class NLI_Jsonl_Reader(DatasetReader):\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_tokens: int = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def dict_to_instance(self, jdata: dict, label: str = None) -> Instance:\n",
    "        p, h, l = jdata[config.pf], jdata[config.hf], jdata[config.lf]\n",
    "        p_tokens = self.tokenizer.tokenize(p)\n",
    "        h_tokens = self.tokenizer.tokenize(h)\n",
    "        if self.max_tokens:\n",
    "            p_tokens = p_tokens[:self.max_tokens]\n",
    "            h_tokens = h_tokens[:self.max_tokens]\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {config.pf: text_field, config.hf: }\n",
    "        if label:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with jsonlines.open(file_path, \"r\") as fo:\n",
    "            for jdata in fo.iter():\n",
    "                yield self.dict_to_instance(jdata)\n",
    "                #yield self.text_to_instance(premise, hypothesis, sentiment)\n",
    "\n",
    "\n",
    "# Instantiate and use the dataset reader to read a file containing the data\n",
    "reader = ClassificationTsvReader()\n",
    "dataset = reader.read('quick_start/data/movie_review/train.tsv')\n",
    "\n",
    "# Returned dataset is a list of Instances by default\n",
    "print('type of dataset: ', type(dataset))\n",
    "print('type of its first element: ', type(dataset[0]))\n",
    "print('size of dataset: ', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = torch.rand((1,2)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt2 = torch.ones((1,2), dtype=torch.long)\n",
    "gt2.device.type              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda torch.float32\n"
     ]
    }
   ],
   "source": [
    "gt2 = gt2.to(gt.device)\n",
    "gt2 = gt2.to(gt.dtype)\n",
    "print(gt2.device.type, gt2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
